{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking out cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os as os\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle as pkl\n",
    "\n",
    "import scipy as scipy\n",
    "import scipy.io as spio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.io.matlab import mat_struct\n",
    "import pickle\n",
    "import random\n",
    "random.seed(666)\n",
    "\n",
    "def loadmat(filename):\n",
    "    '''\n",
    "    this function should be called instead of direct spio.loadmat\n",
    "    as it cures the problem of not properly recovering python dictionaries\n",
    "    from mat files. It calls the function check keys to cure all entries\n",
    "    which are still mat-objects\n",
    "    '''\n",
    "    data = spio.loadmat(filename, struct_as_record=True, squeeze_me=True)\n",
    "    return _check_keys(data)\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], scipy.io.matlab.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, scipy.io.matlab.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load session data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {}\n",
    "meta[45] = {'mouse':'3C280', 'ks':'Kilosort_2024-04-12_162032', 'del_units':[594], 'vis_mice':np.array(['nan', 'beta', 'alpha'])}\n",
    "meta[46] = {'mouse':'3C280', 'ks':'Kilosort_2024-04-12_180855', 'del_units':None, 'vis_mice':np.array(['nan', 'alpha', 'beta'])}\n",
    "meta[11] = {'mouse':'3C290', 'ks':'Kilosort_2024-05-06_154258', 'del_units':[847, 835], 'vis_mice':np.array(['nan', 'beta', 'alpha'])}\n",
    "meta[13] = {'mouse':'3C290', 'ks':'Kilosort_2024-05-14_120055', 'del_units':None, 'vis_mice':np.array(['nan', 'beta', 'alpha'])}\n",
    "meta[14] = {'mouse':'3C290', 'ks':'Kilosort_2024-05-14_122629', 'del_units':None, 'vis_mice':np.array(['nan', 'beta', 'alpha'])}\n",
    "meta[19] = {'mouse':'3C290', 'ks':'Kilosort_2024-05-14_140410', 'del_units':None, 'vis_mice':np.array(['nan', 'beta', 'alpha'])}\n",
    "meta[20] = {'mouse':'3C290', 'ks':'Kilosort_2024-05-15_110539', 'del_units':[33], 'vis_mice':np.array(['nan', 'beta', 'alpha'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse = meta[Session]['mouse']\n",
    "ks = meta[Session]['ks']\n",
    "del_units=meta[Session]['del_units']\n",
    "vis_mice = meta[Session]['vis_mice']\n",
    "active_blocks = [1,2,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_idx = np.where(vis_mice=='alpha')[0][0]\n",
    "b_idx = np.where(vis_mice=='beta')[0][0]\n",
    "c_idx = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save results\n",
    "try:\n",
    "    os.makedirs(f'C:\\\\Users\\\\ebukina\\\\Desktop\\\\eva\\\\results\\\\taking_out_analysis')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to save results\n",
    "try:\n",
    "    os.makedirs(f'C:\\\\Users\\\\ebukina\\\\Desktop\\\\eva\\\\results\\\\taking_out_analysis\\\\S{Session}')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ephys data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "KSdir = f'L:\\\\everyone\\\\sharedDATA\\\\ProcessedDATA\\\\{mouse}\\\\{mouse}_S{Session}\\\\{ks}\\\\'\n",
    "\n",
    "spiketimesfile = KSdir+\"spike_times.npy\"  \n",
    "spiketimes = np.load(spiketimesfile)  #### all spiketimes as indexes regardless of cluster\n",
    "\n",
    "clusterfile = KSdir+\"spike_clusters.npy\"\n",
    "spikeclusters = np.load(clusterfile) #### cluster id for each detected spike\n",
    "\n",
    "Clusterinfofile = KSdir+\"cluster_info.tsv\"\n",
    "Clusterinfo = pd.read_csv(Clusterinfofile,sep='\\t') #### cluster meta-data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 45: 24 good clusters\n"
     ]
    }
   ],
   "source": [
    "goodclusts = Clusterinfo['cluster_id'][np.where(Clusterinfo['group']=='good')[0]]\n",
    "goods = []\n",
    "for clust in goodclusts :\n",
    "    goods.append(clust)\n",
    "print(f'Session {Session}: {len(goods)} good clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodspiketimes = {}\n",
    "\n",
    "spikethresh = 1000 # min nb of spikes\n",
    "for goodunit in goods :\n",
    "    goodinds = np.where(spikeclusters==goodunit)[0]\n",
    "    if goodinds.shape[0] > spikethresh : \n",
    "        goodspiketimes[goodunit] = spiketimes[goodinds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(goodspiketimes.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behavior data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matfile =  f'L:\\\\everyone\\\\sharedDATA\\\\ProcessedDATA\\\\{mouse}\\\\{mouse}_S{Session}\\\\Behaviour.mat'\n",
    "EvaBehavior = loadmat(matfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_behavioral_event(event_idx, time_to_idx = True):\n",
    "    '''\n",
    "    returns dictionary; keys - blocks; inside - array with event :\n",
    "    time stamps in [sec] if time_to_idx = False\n",
    "    indexes if time_to_idx = True\n",
    "    '''\n",
    "    event_dic = {}\n",
    "\n",
    "    for block in np.arange(EvaBehavior['Behaviour'].shape[0]) :\n",
    "        if time_to_idx:\n",
    "            event_dic[block] = EvaBehavior['Behaviour'][block][event_idx]*20000\n",
    "        else:\n",
    "            event_dic[block] = EvaBehavior['Behaviour'][block][event_idx]\n",
    "        event_dic[block] = event_dic[block].astype(int)\n",
    "    \n",
    "    return event_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSocialSampleWindowPerTrial = load_behavioral_event(12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firing rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fr_matrix_prep(beforesamples, aftersamples, blocks, bins, del_units=None):\n",
    "    # discard wierd cells based on rasters\n",
    "    goodspiketimes_copy = copy.deepcopy(goodspiketimes)\n",
    "\n",
    "    try:\n",
    "        for unit in del_units:\n",
    "            del goodspiketimes_copy[unit]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    units = goodspiketimes_copy.keys()\n",
    "    fr_dic = {}\n",
    "   \n",
    "    for block in blocks:\n",
    "        n_trials = TestSocialSampleWindowPerTrial[block].shape[0]\n",
    "        n_units = len(goodspiketimes_copy.keys())\n",
    "\n",
    "        fr_matrix = np.zeros((n_trials, n_units))\n",
    "\n",
    "        i = 0\n",
    "        for unit in units: # list of good cluster_id\n",
    "            \n",
    "            \n",
    "            for event in np.arange(TestSocialSampleWindowPerTrial[block].shape[0]) : #iterate by npoke events\n",
    "\n",
    "                onset = TestSocialSampleWindowPerTrial[block][event,0]\n",
    "                offset = TestSocialSampleWindowPerTrial[block][event,1]\n",
    "                \n",
    "                cond1 = np.where(goodspiketimes_copy[unit].astype(int)>=onset-beforesamples)[0]\n",
    "                cond2 = np.where(goodspiketimes_copy[unit].astype(int)<=offset+aftersamples)[0]\n",
    "                unitspikes = goodspiketimes_copy[unit][np.intersect1d(cond1,cond2)].astype(int)-onset\n",
    "\n",
    "                n = np.histogram(unitspikes, bins=bins)[0][0]\n",
    "                fr = n/0.25\n",
    "                fr_matrix[event, i] = fr\n",
    "            i+=1\n",
    "                \n",
    "        fr_dic[block] = fr_matrix\n",
    "    \n",
    "    return fr_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find spikes in interesting behavioral window\n",
    "samplplimg_rate = 20*1000 #20 kHz\n",
    "beforesamples = 0*samplplimg_rate # 1 sec (expressed in samples for a 20khz sampling rate)\n",
    "aftersamples = 0*samplplimg_rate # 1 sec\n",
    "npoke_window = 0.25\n",
    "eventlength = npoke_window*samplplimg_rate # 0.25 s\n",
    "\n",
    "binsize = 0.25*samplplimg_rate \n",
    "bins = np.linspace(-beforesamples,aftersamples+eventlength,int((beforesamples+aftersamples+eventlength)/binsize)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250 ms during the nosepoke\n",
    "firing_rates_dic = fr_matrix_prep(beforesamples, aftersamples, blocks=[1,2,3], bins=bins, del_units=del_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 23)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firing_rates_dic[a_idx].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy.stats import percentileofscore\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(alpha_data, beta_data, kernel = 'linear', cv=5):\n",
    "\n",
    "    # params\n",
    "    num_rows = alpha_data.shape[0]\n",
    "    indices = list(range(num_rows))\n",
    "    train_proportion = 6 # out of 30 trials\n",
    "\n",
    "    # collect outcomes\n",
    "    accuracy_cv5 = []\n",
    "\n",
    "    for n in range(cv):\n",
    "        test_alpha_idx = random.sample(indices, train_proportion)\n",
    "        train_alpha_idx = [number for number in indices if number not in test_alpha_idx]\n",
    "        test_alpha = alpha_data[test_alpha_idx]\n",
    "        train_alpha = alpha_data[train_alpha_idx]\n",
    "\n",
    "        test_beta_idx = random.sample(indices, train_proportion)\n",
    "        train_beta_idx = [number for number in indices if number not in test_beta_idx]\n",
    "        test_beta = beta_data[test_beta_idx]\n",
    "        train_beta = beta_data[train_beta_idx]\n",
    "\n",
    "        test_ab = np.concatenate((test_alpha,test_beta), axis=0)\n",
    "        train_ab = np.concatenate((train_alpha,train_beta), axis=0)\n",
    "\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        test_ab = scaler.fit_transform(test_ab)\n",
    "        train_ab = scaler.fit_transform(train_ab)\n",
    "\n",
    "        y_test = np.concatenate((np.ones(test_alpha.shape[0]),np.zeros(test_beta.shape[0])))\n",
    "        y_train = np.concatenate((np.ones(train_alpha.shape[0]),np.zeros(train_beta.shape[0])))\n",
    "\n",
    "        # randomize train\n",
    "        num_rows = train_ab.shape[0]\n",
    "        shuffled_indices = list(range(num_rows))\n",
    "        random.shuffle(shuffled_indices)\n",
    "        train_ab = train_ab[shuffled_indices]\n",
    "        y_train = y_train[shuffled_indices]\n",
    "\n",
    "        svm_classifier_cv = SVC(kernel=kernel, random_state=666)\n",
    "        svm_classifier_cv.fit(train_ab, y_train)\n",
    "        y_pred = svm_classifier_cv.predict(test_ab)\n",
    "        accuracy_cv5.append(accuracy_score(y_test, y_pred)) \n",
    "\n",
    "    return np.mean(np.array(accuracy_cv5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 23), (30, 23), (30, 23), (30, 23))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data prep\n",
    "alpha_data = firing_rates_dic[a_idx]\n",
    "beta_data = firing_rates_dic[b_idx]\n",
    "blank_data = firing_rates_dic[c_idx]\n",
    "\n",
    "num_rows = alpha_data.shape[0]\n",
    "shuffled_indices = list(range(num_rows))\n",
    "\n",
    "random.shuffle(shuffled_indices)\n",
    "alpha_data = alpha_data[shuffled_indices]\n",
    "\n",
    "random.shuffle(shuffled_indices)\n",
    "beta_data = beta_data[shuffled_indices]\n",
    "\n",
    "num_rows = alpha_data.shape[0]\n",
    "social_data = np.concatenate((alpha_data[:int(num_rows/2),:], beta_data[int(num_rows/2):,:]), axis=0)\n",
    "\n",
    "random.shuffle(shuffled_indices)\n",
    "blank_data = blank_data[shuffled_indices]\n",
    "\n",
    "alpha_data.shape, beta_data.shape, blank_data.shape, social_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking out neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23,\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neurons = firing_rates_dic[a_idx].shape[1]\n",
    "neurons_idx = np.arange(0,n_neurons,1)\n",
    "n_neurons, neurons_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 23) (30, 23) (30, 23) (30, 23)\n",
      "(30, 23) (30, 23) (30, 23) (30, 23)\n",
      "(30, 23) (30, 23) (30, 23) (30, 23)\n",
      "(30, 23) (30, 23) (30, 23) (30, 23)\n",
      "(30, 22) (30, 22) (30, 22) (30, 22)\n",
      "(30, 22) (30, 22) (30, 22) (30, 22)\n",
      "(30, 22) (30, 22) (30, 22) (30, 22)\n",
      "(30, 22) (30, 22) (30, 22) (30, 22)\n",
      "(30, 21) (30, 21) (30, 21) (30, 21)\n",
      "(30, 21) (30, 21) (30, 21) (30, 21)\n",
      "(30, 21) (30, 21) (30, 21) (30, 21)\n",
      "(30, 21) (30, 21) (30, 21) (30, 21)\n",
      "(30, 20) (30, 20) (30, 20) (30, 20)\n",
      "(30, 20) (30, 20) (30, 20) (30, 20)\n",
      "(30, 20) (30, 20) (30, 20) (30, 20)\n",
      "(30, 20) (30, 20) (30, 20) (30, 20)\n",
      "(30, 19) (30, 19) (30, 19) (30, 19)\n",
      "(30, 19) (30, 19) (30, 19) (30, 19)\n",
      "(30, 19) (30, 19) (30, 19) (30, 19)\n",
      "(30, 19) (30, 19) (30, 19) (30, 19)\n",
      "(30, 18) (30, 18) (30, 18) (30, 18)\n",
      "(30, 18) (30, 18) (30, 18) (30, 18)\n",
      "(30, 18) (30, 18) (30, 18) (30, 18)\n",
      "(30, 18) (30, 18) (30, 18) (30, 18)\n",
      "(30, 17) (30, 17) (30, 17) (30, 17)\n",
      "(30, 17) (30, 17) (30, 17) (30, 17)\n",
      "(30, 17) (30, 17) (30, 17) (30, 17)\n",
      "(30, 17) (30, 17) (30, 17) (30, 17)\n",
      "(30, 16) (30, 16) (30, 16) (30, 16)\n",
      "(30, 16) (30, 16) (30, 16) (30, 16)\n",
      "(30, 16) (30, 16) (30, 16) (30, 16)\n",
      "(30, 16) (30, 16) (30, 16) (30, 16)\n",
      "(30, 15) (30, 15) (30, 15) (30, 15)\n",
      "(30, 15) (30, 15) (30, 15) (30, 15)\n",
      "(30, 15) (30, 15) (30, 15) (30, 15)\n",
      "(30, 15) (30, 15) (30, 15) (30, 15)\n",
      "(30, 14) (30, 14) (30, 14) (30, 14)\n",
      "(30, 14) (30, 14) (30, 14) (30, 14)\n",
      "(30, 14) (30, 14) (30, 14) (30, 14)\n",
      "(30, 14) (30, 14) (30, 14) (30, 14)\n",
      "(30, 13) (30, 13) (30, 13) (30, 13)\n",
      "(30, 13) (30, 13) (30, 13) (30, 13)\n",
      "(30, 13) (30, 13) (30, 13) (30, 13)\n",
      "(30, 13) (30, 13) (30, 13) (30, 13)\n",
      "(30, 12) (30, 12) (30, 12) (30, 12)\n",
      "(30, 12) (30, 12) (30, 12) (30, 12)\n",
      "(30, 12) (30, 12) (30, 12) (30, 12)\n",
      "(30, 12) (30, 12) (30, 12) (30, 12)\n",
      "(30, 11) (30, 11) (30, 11) (30, 11)\n",
      "(30, 11) (30, 11) (30, 11) (30, 11)\n",
      "(30, 11) (30, 11) (30, 11) (30, 11)\n",
      "(30, 11) (30, 11) (30, 11) (30, 11)\n",
      "(30, 10) (30, 10) (30, 10) (30, 10)\n",
      "(30, 10) (30, 10) (30, 10) (30, 10)\n",
      "(30, 10) (30, 10) (30, 10) (30, 10)\n",
      "(30, 10) (30, 10) (30, 10) (30, 10)\n",
      "(30, 9) (30, 9) (30, 9) (30, 9)\n",
      "(30, 9) (30, 9) (30, 9) (30, 9)\n",
      "(30, 9) (30, 9) (30, 9) (30, 9)\n",
      "(30, 9) (30, 9) (30, 9) (30, 9)\n",
      "(30, 8) (30, 8) (30, 8) (30, 8)\n",
      "(30, 8) (30, 8) (30, 8) (30, 8)\n",
      "(30, 8) (30, 8) (30, 8) (30, 8)\n",
      "(30, 8) (30, 8) (30, 8) (30, 8)\n",
      "(30, 7) (30, 7) (30, 7) (30, 7)\n",
      "(30, 7) (30, 7) (30, 7) (30, 7)\n",
      "(30, 7) (30, 7) (30, 7) (30, 7)\n",
      "(30, 7) (30, 7) (30, 7) (30, 7)\n",
      "(30, 6) (30, 6) (30, 6) (30, 6)\n",
      "(30, 6) (30, 6) (30, 6) (30, 6)\n",
      "(30, 6) (30, 6) (30, 6) (30, 6)\n",
      "(30, 6) (30, 6) (30, 6) (30, 6)\n",
      "(30, 5) (30, 5) (30, 5) (30, 5)\n",
      "(30, 5) (30, 5) (30, 5) (30, 5)\n",
      "(30, 5) (30, 5) (30, 5) (30, 5)\n",
      "(30, 5) (30, 5) (30, 5) (30, 5)\n",
      "(30, 4) (30, 4) (30, 4) (30, 4)\n",
      "(30, 4) (30, 4) (30, 4) (30, 4)\n",
      "(30, 4) (30, 4) (30, 4) (30, 4)\n",
      "(30, 4) (30, 4) (30, 4) (30, 4)\n",
      "(30, 3) (30, 3) (30, 3) (30, 3)\n",
      "(30, 3) (30, 3) (30, 3) (30, 3)\n",
      "(30, 3) (30, 3) (30, 3) (30, 3)\n",
      "(30, 3) (30, 3) (30, 3) (30, 3)\n",
      "(30, 2) (30, 2) (30, 2) (30, 2)\n",
      "(30, 2) (30, 2) (30, 2) (30, 2)\n",
      "(30, 2) (30, 2) (30, 2) (30, 2)\n",
      "(30, 2) (30, 2) (30, 2) (30, 2)\n",
      "(30, 1) (30, 1) (30, 1) (30, 1)\n",
      "(30, 1) (30, 1) (30, 1) (30, 1)\n",
      "(30, 1) (30, 1) (30, 1) (30, 1)\n",
      "(30, 1) (30, 1) (30, 1) (30, 1)\n"
     ]
    }
   ],
   "source": [
    "SVM_s_vs_bl = []\n",
    "SVM_a_vs_b = []\n",
    "\n",
    "for n_take in range(n_neurons): # take out from 0 to n-1 neurons\n",
    "\n",
    "    SVM_sbl = []\n",
    "    SVM_ab = []\n",
    "\n",
    "    mew=True\n",
    "    \n",
    "    for ch in range(100):\n",
    "        \n",
    "        # remove columns (neurons)\n",
    "        take_idx = random.sample(list(neurons_idx), k=n_take)\n",
    "        a_matrix = np.delete(alpha_data, take_idx, axis=1)\n",
    "        b_matrix = np.delete(beta_data, take_idx, axis=1)\n",
    "        bl_matrix = np.delete(blank_data, take_idx, axis=1)\n",
    "        s_matrix = np.delete(social_data, take_idx, axis=1)\n",
    "\n",
    "        if ch<=3:\n",
    "            print(a_matrix.shape, b_matrix.shape, bl_matrix.shape, s_matrix.shape)\n",
    "\n",
    "        svm_ab = SVM(a_matrix, b_matrix)\n",
    "        svm_sbl = SVM(bl_matrix, s_matrix)\n",
    "\n",
    "        SVM_ab.append(svm_ab)\n",
    "        SVM_sbl.append(svm_sbl)\n",
    "        \n",
    "\n",
    "    SVM_a_vs_b.append(np.mean(np.array(SVM_ab)))\n",
    "    SVM_s_vs_bl.append(np.mean(np.array(SVM_sbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trials = firing_rates_dic[a_idx].shape[0]\n",
    "trials_idx = np.arange(0,n_trials,1)\n",
    "trials_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 23) (30, 23) (30, 23) (30, 23)\n",
      "(30, 23) (30, 23) (30, 23) (30, 23)\n",
      "(30, 23) (30, 23) (30, 23) (30, 23)\n",
      "(29, 23) (29, 23) (29, 23) (29, 23)\n",
      "(29, 23) (29, 23) (29, 23) (29, 23)\n",
      "(29, 23) (29, 23) (29, 23) (29, 23)\n",
      "(28, 23) (28, 23) (28, 23) (28, 23)\n",
      "(28, 23) (28, 23) (28, 23) (28, 23)\n",
      "(28, 23) (28, 23) (28, 23) (28, 23)\n",
      "(27, 23) (27, 23) (27, 23) (27, 23)\n",
      "(27, 23) (27, 23) (27, 23) (27, 23)\n",
      "(27, 23) (27, 23) (27, 23) (27, 23)\n",
      "(26, 23) (26, 23) (26, 23) (26, 23)\n",
      "(26, 23) (26, 23) (26, 23) (26, 23)\n",
      "(26, 23) (26, 23) (26, 23) (26, 23)\n",
      "(25, 23) (25, 23) (25, 23) (25, 23)\n",
      "(25, 23) (25, 23) (25, 23) (25, 23)\n",
      "(25, 23) (25, 23) (25, 23) (25, 23)\n",
      "(24, 23) (24, 23) (24, 23) (24, 23)\n",
      "(24, 23) (24, 23) (24, 23) (24, 23)\n",
      "(24, 23) (24, 23) (24, 23) (24, 23)\n",
      "(23, 23) (23, 23) (23, 23) (23, 23)\n",
      "(23, 23) (23, 23) (23, 23) (23, 23)\n",
      "(23, 23) (23, 23) (23, 23) (23, 23)\n",
      "(22, 23) (22, 23) (22, 23) (22, 23)\n",
      "(22, 23) (22, 23) (22, 23) (22, 23)\n",
      "(22, 23) (22, 23) (22, 23) (22, 23)\n",
      "(21, 23) (21, 23) (21, 23) (21, 23)\n",
      "(21, 23) (21, 23) (21, 23) (21, 23)\n",
      "(21, 23) (21, 23) (21, 23) (21, 23)\n",
      "(20, 23) (20, 23) (20, 23) (20, 23)\n",
      "(20, 23) (20, 23) (20, 23) (20, 23)\n",
      "(20, 23) (20, 23) (20, 23) (20, 23)\n",
      "(19, 23) (19, 23) (19, 23) (19, 23)\n",
      "(19, 23) (19, 23) (19, 23) (19, 23)\n",
      "(19, 23) (19, 23) (19, 23) (19, 23)\n",
      "(18, 23) (18, 23) (18, 23) (18, 23)\n",
      "(18, 23) (18, 23) (18, 23) (18, 23)\n",
      "(18, 23) (18, 23) (18, 23) (18, 23)\n",
      "(17, 23) (17, 23) (17, 23) (17, 23)\n",
      "(17, 23) (17, 23) (17, 23) (17, 23)\n",
      "(17, 23) (17, 23) (17, 23) (17, 23)\n",
      "(16, 23) (16, 23) (16, 23) (16, 23)\n",
      "(16, 23) (16, 23) (16, 23) (16, 23)\n",
      "(16, 23) (16, 23) (16, 23) (16, 23)\n",
      "(15, 23) (15, 23) (15, 23) (15, 23)\n",
      "(15, 23) (15, 23) (15, 23) (15, 23)\n",
      "(15, 23) (15, 23) (15, 23) (15, 23)\n",
      "(14, 23) (14, 23) (14, 23) (14, 23)\n",
      "(14, 23) (14, 23) (14, 23) (14, 23)\n",
      "(14, 23) (14, 23) (14, 23) (14, 23)\n",
      "(13, 23) (13, 23) (13, 23) (13, 23)\n",
      "(13, 23) (13, 23) (13, 23) (13, 23)\n",
      "(13, 23) (13, 23) (13, 23) (13, 23)\n",
      "(12, 23) (12, 23) (12, 23) (12, 23)\n",
      "(12, 23) (12, 23) (12, 23) (12, 23)\n",
      "(12, 23) (12, 23) (12, 23) (12, 23)\n",
      "(11, 23) (11, 23) (11, 23) (11, 23)\n",
      "(11, 23) (11, 23) (11, 23) (11, 23)\n",
      "(11, 23) (11, 23) (11, 23) (11, 23)\n",
      "(10, 23) (10, 23) (10, 23) (10, 23)\n",
      "(10, 23) (10, 23) (10, 23) (10, 23)\n",
      "(10, 23) (10, 23) (10, 23) (10, 23)\n",
      "(9, 23) (9, 23) (9, 23) (9, 23)\n",
      "(9, 23) (9, 23) (9, 23) (9, 23)\n",
      "(9, 23) (9, 23) (9, 23) (9, 23)\n",
      "(8, 23) (8, 23) (8, 23) (8, 23)\n",
      "(8, 23) (8, 23) (8, 23) (8, 23)\n",
      "(8, 23) (8, 23) (8, 23) (8, 23)\n",
      "(7, 23) (7, 23) (7, 23) (7, 23)\n",
      "(7, 23) (7, 23) (7, 23) (7, 23)\n",
      "(7, 23) (7, 23) (7, 23) (7, 23)\n"
     ]
    }
   ],
   "source": [
    "tSVM_s_vs_bl = []\n",
    "tSVM_a_vs_b = []\n",
    "\n",
    "for n_take in range(n_trials-6): # we need at least 7 trials since test=6\n",
    "\n",
    "    SVM_sbl = []\n",
    "    SVM_ab = []\n",
    "    mew=True\n",
    "\n",
    "    for ch in range(100):\n",
    "        # remove rows (trials)\n",
    "\n",
    "        take_idx = random.sample(list(trials_idx), k=n_take)\n",
    "        a_matrix = np.delete(alpha_data, take_idx, axis=0)\n",
    "        b_matrix = np.delete(beta_data, take_idx, axis=0)\n",
    "        bl_matrix = np.delete(blank_data, take_idx, axis=0)\n",
    "        s_matrix = np.delete(social_data, take_idx, axis=0)\n",
    "    \n",
    "        if ch<3:\n",
    "            print(a_matrix.shape, b_matrix.shape, bl_matrix.shape, s_matrix.shape)\n",
    "\n",
    "        svm_ab = SVM(a_matrix, b_matrix)\n",
    "        svm_sbl = SVM(bl_matrix, s_matrix)\n",
    "\n",
    "        SVM_ab.append(svm_ab)\n",
    "        SVM_sbl.append(svm_sbl)\n",
    "\n",
    "    tSVM_a_vs_b.append(np.mean(np.array(SVM_ab)))\n",
    "    tSVM_s_vs_bl.append(np.mean(np.array(SVM_sbl)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_take_out_dic = {}\n",
    "SVM_take_out_dic['SVM_s_vs_bl'] = SVM_s_vs_bl\n",
    "SVM_take_out_dic['SVM_a_vs_b'] = SVM_a_vs_b\n",
    "SVM_take_out_dic['tSVM_s_vs_bl'] = tSVM_s_vs_bl\n",
    "SVM_take_out_dic['tSVM_a_vs_b'] = tSVM_a_vs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'C:\\\\Users\\\\ebukina\\\\Desktop\\\\eva\\\\results\\\\taking_out_analysis\\\\S{Session}\\\\{mouse}_S{Session}_SVM_take_out_dic.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(SVM_take_out_dic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 23)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_data = np.concatenate((alpha_data,beta_data,blank_data), axis=0)\n",
    "concat_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90,\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "        68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
       "        85, 86, 87, 88, 89]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_rows = concat_data.shape[0]\n",
    "rows_idx = np.arange(0,n_rows,1)\n",
    "n_rows, rows_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = list(range(n_rows))\n",
    "random.shuffle(shuffled_indices)\n",
    "concat_data_sh = concat_data[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m s_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdelete(social_matrix, take_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m svm_ab \u001b[38;5;241m=\u001b[39m SVM(a_matrix, b_matrix)\n\u001b[1;32m---> 34\u001b[0m svm_sbl \u001b[38;5;241m=\u001b[39m SVM(bl_matrix, s_matrix)\n\u001b[0;32m     36\u001b[0m SVM_ab\u001b[38;5;241m.\u001b[39mappend(svm_ab)\n\u001b[0;32m     37\u001b[0m SVM_sbl\u001b[38;5;241m.\u001b[39mappend(svm_sbl)\n",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m, in \u001b[0;36mSVM\u001b[1;34m(alpha_data, beta_data, kernel, cv)\u001b[0m\n\u001b[0;32m     26\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m     27\u001b[0m test_ab \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(test_ab)\n\u001b[1;32m---> 28\u001b[0m train_ab \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(train_ab)\n\u001b[0;32m     30\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mones(test_alpha\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),np\u001b[38;5;241m.\u001b[39mzeros(test_beta\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])))\n\u001b[0;32m     31\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39mones(train_alpha\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),np\u001b[38;5;241m.\u001b[39mzeros(train_beta\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])))\n",
      "File \u001b[1;32mc:\\Users\\ebukina\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ebukina\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\ebukina\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\ebukina\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:957\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    946\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvar_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m _incremental_mean_and_var(\n\u001b[0;32m    947\u001b[0m             X,\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    951\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    952\u001b[0m         )\n\u001b[0;32m    954\u001b[0m \u001b[38;5;66;03m# for backward-compatibility, reduce n_samples_seen_ to an integer\u001b[39;00m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;66;03m# if the number of samples is the same for each feature (i.e. no\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;66;03m# missing values)\u001b[39;00m\n\u001b[1;32m--> 957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mptp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Extract the list of near constant features on the raw variances,\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;66;03m# before taking the square root.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ebukina\\AppData\\Local\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2589\u001b[0m, in \u001b[0;36m_ptp_dispatcher\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;124;03m    Return the cumulative sum of the elements along a given axis.\u001b[39;00m\n\u001b[0;32m   2516\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2584\u001b[0m \n\u001b[0;32m   2585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumsum\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout)\n\u001b[1;32m-> 2589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ptp_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[0;32m   2593\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_ptp_dispatcher)\n\u001b[0;32m   2594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mptp\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sh_neurons_sbl_list = []\n",
    "sh_neurons_ab_list = []\n",
    "\n",
    "for sh in range(100):\n",
    "    # making shuffle data\n",
    "    take_idx = random.sample(list(rows_idx), k=30)\n",
    "    alpha_matrix = concat_data_sh[take_idx]\n",
    "    take_idx = random.sample(list(rows_idx), k=30)\n",
    "    beta_matrix = concat_data_sh[take_idx]\n",
    "    take_idx = random.sample(list(rows_idx), k=30)\n",
    "    social_matrix = concat_data_sh[take_idx]\n",
    "    take_idx = random.sample(list(rows_idx), k=30)\n",
    "    blank_matrix = concat_data_sh[take_idx]\n",
    "\n",
    "    SVM_s_vs_bl = []\n",
    "    SVM_a_vs_b = []\n",
    "\n",
    "\n",
    "    for n_take in range(n_neurons): # take out from 0 to n-1 neurons\n",
    "\n",
    "        SVM_sbl = []\n",
    "        SVM_ab = []\n",
    "        \n",
    "        for ch in range(100):\n",
    "            \n",
    "            # remove columns (neurons)\n",
    "            take_idx = random.sample(list(neurons_idx), k=n_take)\n",
    "            a_matrix = np.delete(alpha_matrix, take_idx, axis=1)\n",
    "            b_matrix = np.delete(beta_matrix, take_idx, axis=1)\n",
    "            bl_matrix = np.delete(blank_matrix, take_idx, axis=1)\n",
    "            s_matrix = np.delete(social_matrix, take_idx, axis=1)\n",
    "\n",
    "            svm_ab = SVM(a_matrix, b_matrix)\n",
    "            svm_sbl = SVM(bl_matrix, s_matrix)\n",
    "\n",
    "            SVM_ab.append(svm_ab)\n",
    "            SVM_sbl.append(svm_sbl)\n",
    "            \n",
    "        SVM_s_vs_bl.append(np.mean(np.array(SVM_sbl)))\n",
    "        SVM_a_vs_b.append(np.mean(np.array(SVM_ab)))\n",
    "\n",
    "    sh_neurons_sbl_list.append(SVM_s_vs_bl)\n",
    "    sh_neurons_ab_list.append(SVM_a_vs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete trials\n",
    "sh_trials_sbl_list = []\n",
    "sh_trils_ab_list = []\n",
    "\n",
    "for sh in range(100):\n",
    "    # making shuffle data\n",
    "    take_idx = random.sample(list(rows_idx), k=30)\n",
    "    alpha_matrix = concat_data_sh[take_idx]\n",
    "    take_idx = random.sample(list(rows_idx), k=30)\n",
    "    beta_matrix = concat_data_sh[take_idx]\n",
    "    take_idx = random.sample(list(rows_idx), k=30)\n",
    "    social_matrix = concat_data_sh[take_idx]\n",
    "    take_idx = random.sample(list(rows_idx), k=30)\n",
    "    blank_matrix = concat_data_sh[take_idx]\n",
    "\n",
    "    SVM_s_vs_bl = []\n",
    "    SVM_a_vs_b = []\n",
    "\n",
    "\n",
    "    for n_take in range(n_trials-6): # take out from 0 to n-1 neurons\n",
    "\n",
    "        SVM_sbl = []\n",
    "        SVM_ab = []\n",
    "        \n",
    "        for ch in range(100):\n",
    "            \n",
    "            # remove columns (neurons)\n",
    "            take_idx = random.sample(list(trials_idx), k=n_take)\n",
    "            a_matrix = np.delete(alpha_matrix, take_idx, axis=0)\n",
    "            b_matrix = np.delete(beta_matrix, take_idx, axis=0)\n",
    "            bl_matrix = np.delete(blank_matrix, take_idx, axis=0)\n",
    "            s_matrix = np.delete(social_matrix, take_idx, axis=0)\n",
    "\n",
    "            svm_ab = SVM(a_matrix, b_matrix)\n",
    "            svm_sbl = SVM(bl_matrix, s_matrix)\n",
    "\n",
    "            SVM_ab.append(svm_ab)\n",
    "            SVM_sbl.append(svm_sbl)\n",
    "            \n",
    "        SVM_s_vs_bl.append(np.mean(np.array(SVM_sbl)))\n",
    "        SVM_a_vs_b.append(np.mean(np.array(SVM_ab)))\n",
    "\n",
    "    sh_trials_sbl_list.append(SVM_s_vs_bl)\n",
    "    sh_trils_ab_list.append(SVM_a_vs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_dic = {}\n",
    "sh_dic['sh_neurons_sbl_list'] = sh_neurons_sbl_list\n",
    "sh_dic['sh_neurons_ab_list'] = sh_neurons_ab_list\n",
    "sh_dic['sh_trials_sbl_list'] = sh_trials_sbl_list\n",
    "sh_dic['sh_trils_ab_list'] = sh_trils_ab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'C:\\\\Users\\\\ebukina\\\\Desktop\\\\eva\\\\results\\\\taking_out_analysis\\\\S{Session}\\\\{mouse}_S{Session}_sh_dic.pkl'\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(sh_dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
